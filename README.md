# Math Tutor Agent - Domain-Specific LLM Implementation

## ğŸ¯ Project Overview
A domain-specific LLM agent designed for math tutoring (Grades 6-10) using advanced prompt engineering strategies. This project demonstrates various prompting techniques and evaluates their effectiveness in educational contexts.

## ğŸ“‹ Assignment Context
**Course**: Prompt Engineering Lab  
**Objective**: Building and Evaluating Domain-Specific LLM Agents  
**Domain**: EdTech Math Tutor for Class 6â€“10  

## ğŸ—ï¸ Project Structure
```
â”œâ”€â”€ README.md (project overview, setup instructions, findings)
â”œâ”€â”€ domain_analysis.md (understanding of domain tasks)
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ zero_shot.txt
â”‚   â”œâ”€â”€ few_shot.txt
â”‚   â”œâ”€â”€ cot_prompt.txt
â”‚   â””â”€â”€ meta_prompt.txt
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ input_queries.json
â”‚   â”œâ”€â”€ output_logs.json
â”‚   â””â”€â”€ analysis_report.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py or notebook.ipynb
â”‚   â””â”€â”€ utils.py (optional helpers)
â””â”€â”€ hallucination_log.md (examples of failure cases)
```

## ğŸš€ Setup Instructions

### Prerequisites
- Python 3.8+
- Local LLM setup (Ollama recommended)

### Installation
1. **Install Ollama**
   ```bash
   # Visit https://ollama.ai/ and install
   ollama pull mistral
   # or alternatively
   ollama pull llama2
   ```

2. **Install Python Dependencies**
   ```bash
   pip install requests json datetime typing
   ```

3. **Clone and Run**
   ```bash
   python src/main.py
   ```

## ğŸ¯ Domain Tasks Covered
1. **Problem Solving**: Step-by-step mathematical solutions
2. **Concept Explanation**: Breaking down math concepts for students
3. **Error Analysis**: Identifying and correcting student mistakes

## ğŸ“Š Prompt Strategies Implemented
- **Zero-shot**: Direct instruction prompting
- **Few-shot**: Example-based learning
- **Chain-of-Thought (CoT)**: Step-by-step reasoning
- **Meta-prompting**: Self-questioning approach

## ğŸ§ª Evaluation Metrics
- **Accuracy**: Correctness of mathematical solutions
- **Reasoning Clarity**: Quality of step-by-step explanations
- **Hallucination Score**: Detection of mathematical errors
- **Consistency**: Reliability across similar problems

## ğŸ“ˆ Key Findings
- Chain-of-thought prompts showed highest accuracy for complex problems
- Few-shot examples improved consistency across problem types
- Meta-prompting excelled in word problem interpretation
- Hallucinations mainly occurred in advanced geometry concepts

## ğŸ”§ Usage
```python
from src.main import MathTutorAgent

# Initialize agent
tutor = MathTutorAgent()

# Run comprehensive evaluation
tutor.run_comprehensive_test()

# Generate detailed report
tutor.generate_report()
```

## ğŸ“ File Descriptions
- `src/main.py`: Core agent implementation
- `prompts/`: Individual prompt strategy templates
- `evaluation/`: Test queries, results, and analysis
- `domain_analysis.md`: Detailed domain understanding
- `hallucination_log.md`: Documentation of failure cases

## ğŸ“ Assignment Completion
âœ… Domain-specific agent (Math Tutor)  
âœ… Multiple prompt strategies implemented  
âœ… Automated evaluation framework  
âœ… Hallucination detection and logging  
âœ… Comparative analysis of prompt effectiveness  
âœ… Complete documentation and findings  

## ğŸ¤ Contributing
This is an academic assignment project. For improvements or suggestions, please refer to the evaluation reports and analysis documentation.

## ğŸ“„ License
Academic use only - Part of Prompt Engineering coursework.