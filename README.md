# Math Tutor Agent - Domain-Specific LLM Implementation

## ğŸ¯ Project Overview

A domain-specific LLM agent designed for math tutoring (Grades 6-10) using advanced prompt engineering strategies. This project demonstrates various prompting techniques and evaluates their effectiveness in educational contexts.

## ğŸ“‹ Assignment Context

**Course**: Prompt Engineering Lab
**Objective**: Building and Evaluating Domain-Specific LLM Agents
**Domain**: EdTech Math Tutor for Class 6â€“10

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ README.md (project overview, setup instructions, findings)
â”œâ”€â”€ domain_analysis.md (understanding of domain tasks)
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ zero_shot.txt
â”‚   â”œâ”€â”€ few_shot.txt
â”‚   â”œâ”€â”€ cot_prompt.txt
â”‚   â””â”€â”€ meta_prompt.txt
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ input_queries.json
â”‚   â”œâ”€â”€ output_logs.json
â”‚   â””â”€â”€ analysis_report.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py or notebook.ipynb
â”‚   â””â”€â”€ utils.py (optional helpers)
â””â”€â”€ hallucination_log.md (examples of failure cases)
```

## ğŸš€ Setup Instructions

### Prerequisites

* Python 3.8+
* Hugging Face Transformers
* Local LLM (TinyLlama-1.1B-Chat) downloaded

### Installation

1. **Install Required Python Libraries**

   ```bash
   pip install transformers torch
   ```

2. **Ensure Local Model is Available**

   * This project uses:

     ```
     google/flan-t5-small
     ```
   * Download it beforehand via:

     ```python
     from transformers import AutoModelForCausalLM, AutoTokenizer
     tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
     model = AutoModelForCausalLM.from_pretrained("google/flan-t5-small")
     ```

3. **Run the Agent**

   ```bash
   python src/main.py
   ```

## ğŸŒŸ Domain Tasks Covered

1. **Problem Solving**: Step-by-step mathematical solutions
2. **Concept Explanation**: Breaking down math concepts for students
3. **Error Analysis**: Identifying and correcting student mistakes

## ğŸ“Š Prompt Strategies Implemented

* **Zero-shot**: Direct instruction prompting
* **Few-shot**: Example-based learning
* **Chain-of-Thought (CoT)**: Step-by-step reasoning
* **Meta-prompting**: Self-questioning approach

## ğŸ§ª Evaluation Metrics

* **Accuracy**: Correctness of mathematical solutions
* **Reasoning Clarity**: Quality of step-by-step explanations
* **Hallucination Score**: Detection of mathematical errors
* **Consistency**: Reliability across similar problems
* **Age Appropriateness**: Grade-level suitability

## ğŸ“ˆ Key Findings

* Chain-of-thought prompts showed highest accuracy for complex problems
* Few-shot examples improved consistency across problem types
* Meta-prompting excelled in word problem interpretation
* Hallucinations mainly occurred in advanced geometry concepts

## ğŸ”§ Usage

```python
from src.main import MathTutorAgent

# Initialize agent
tutor = MathTutorAgent()

# Run full evaluation
tutor.run_comprehensive_evaluation()
```

## ğŸ“ File Descriptions

* `src/main.py`: Core agent implementation
* `prompts/`: Individual prompt strategy templates
* `evaluation/`: Test queries, results, and analysis
* `domain_analysis.md`: Detailed domain understanding
* `hallucination_log.md`: Documentation of failure cases

## ğŸ“ Assignment Completion

âœ… Domain-specific agent (Math Tutor)
âœ… Multiple prompt strategies implemented
âœ… Automated evaluation framework
âœ… Hallucination detection and logging
âœ… Comparative analysis of prompt effectiveness
âœ… Complete documentation and findings